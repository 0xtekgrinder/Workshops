{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résoud *L-Antique Maze* et apprend les bases du reinforcement learning !\n",
    "```\n",
    "     \n",
    "                                           ..,,;;;;;;,,,,\n",
    "                                     .,;'';;,..,;;;,,,,,.''';;,..\n",
    "                                  ,,''                    '';;;;,;''\n",
    "                                 ;'    ,;@@;'  ,@@;, @@, ';;;@@;,;';.\n",
    "                                ''  ,;@@@@@'  ;@@@@; ''    ;;@@@@@;;;;\n",
    "                                   ;;@@@@@;    '''     .,,;;;@@@@@@@;;;\n",
    "                                  ;;@@@@@@;           , ';;;@@@@@@@@;;;.\n",
    "                                   '';@@@@@,.  ,   .   ',;;;@@@@@@;;;;;;\n",
    "                                      .   '';;;;;;;;;,;;;;@@@@@;;' ,.:;'\n",
    "                                        ''..,,     ''''    '  .,;'\n",
    "                                             ''''''::''''''''\n",
    "                                                                 ,;\n",
    "                                {L-Antique}                     .;;\n",
    "                                                               ,;;;\n",
    "                                Author Slo - Slohan.S        ,;;;;:\n",
    "                                                          ,;@@   .;\n",
    "                                                         ;;@@'  ,;\n",
    "                                                         ';;, ,;'        [17/03/2020]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour la suite de ce Workshop j'utiliserais quelques abréviation :\n",
    ">RL\n",
    ": *Reinforcement Learning*  \n",
    ">DL\n",
    ": *Deep Learning*  \n",
    ">MDP\n",
    ": *Markov Decision Process*\n",
    "\n",
    "Le Reinforcement Learning est une sous-couche du machine learning, en matière d’apprentissage automatisé, on oppose très fréquemment l'apprentissage supervisé et l'apprentissage non supervisé. Le RL lui fait parti du troisième camp qui se situe entre le supervisé et non-supervisé. D'un coté, il utilise des éléments du supervisé tel que les réseaux de neurones du Deep Learning pour apprendre à partir de données. Et d'un autre coté, il n'utilise aucune donnée labellisé pour faire cet apprentissage, l'apprentissage à partir des données se fait donc d'une manière différente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby_img](./img/supervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si je devais vous donner une représentation abstraite du RL, je vous dirais ceci : \n",
    "> Le reinforcement Learning se compose d'un **agent** d'un **environnement** et de **récompenses**. Prenons un exemple simple. Imaginons un bébé dans une maison avec ses parents. Le bébé représenterait l'agent et la maison et tout ce qui est autour tel que les parents représenterait l'environnement. Le bébé possède un **cerveau vierge**, il ne connaît rien, mais possède 5 sens (la vue, l'ouïe, le touché ...) pour lui permettre **d'observer** l'environnement (il est important de noter que l'agent ne peut avoir qu'une observation de l'environnement et pas un aperçu total à chaque fois.). Il va prendre des **actions** aléatoires et attendre les récompenses venant de l'environnement. S'il touche du feu, il aura **mal** ce qui équivaut à une mauvaise récompense. S'il se met debout et que ses parent **applaudissent**, ce serait une récompense positive pour lui.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby_img](./img/baby_r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fondation théorique du Reinforcement Learning \n",
    "\n",
    "\n",
    "Vous avez eu un aperçu de ce qu'est le Reinforcement Learning. Maintenant nous allons voir les fondations théoriques qui se cache derrière et qui vont vous permettre de comprendre en détails cette technique. Dans cette section, nous allons voir ce qu'est un processus de décision markovien. Cela représente les fondamentaux et la théorie sur quoi le RL se tient.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Markov Decision processes (MDP)\n",
    "\n",
    "Avant de couvrir le processus de décision markovien complet nous allons commencer par le cas le plus simple qui est le **processus de markov** basique. Puis on étendra ce processus avec les récompense pour arriver au **processus de récompenses markovien** et enfin, on finira par rajouter la dernière enveloppe : les actions. Ce qui va nous mener au **Processus de décision markovien**. Ces fondations théoriques sont utilisées dans de nombreux problèmes de computer science et ne vous donneront pas qu'une base sur le Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Markov Process\n",
    "\n",
    "Le processus de Markov (aussi appelé Chaîne de markov) est un processus dans lequel vous ne pouvez qu'observer. Imaginez que vous ayez un system devant vous et que vous observiez ce system en ayant aucune influence sur celui-ci. Ce que vous observez est appelé **l'état du system** et le system peu passer d'un état à un autre en fonction de certaines règles. Encore une fois, je le répète vous n'êtes qu'un observateur.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple est toujours plus parlant donc imaginez un robot se déplaçant dans un labyrinthe de 9 cases. Vous êtes un observateur qui observe le déplacement du robot à chaque instant t. Le robot et le labyrinthe représentent le system et l'état du sytem représente la position du robot dans le labyrinthe à un instant t. Le robot bouge à chaque instant de case en case (et donc d'état en état) en suivant des règles mais vous n'avez aucune influence sur le déplacement d'un état t à un état t+1. \n",
    "\n",
    "Tous les états possibles d'un system sont appelés **state space**. Dans l'exemple que nous avons pris le *state space* est de 9 car il y a 9 cases dans le labyrinthe. Ce *state space* doit être une donnée finie, mais peut être extrêmement large. Ci-dessous, on a représenté le changement d'état qu'on à observé au cours du temps. Une séquence d'observations au cours du temps est appelée *chain of state* et forme ce qu'on appelle un **historique**. \n",
    "![baby_img](./img/maze.gif)![transition_img](./img/transition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous l'avez vu, on représente souvent le passage d'état (globalement les MDP) par un graphe avec des nodes.\n",
    "la probabilité de passé d'un état à un autre est définie par le system et l'observateur n'a pas accès à ces probabilités exactes. Il observe simplement le system et peu approximé ces probabilités.\n",
    "Plus on a d'observations plus notre approximation sera proche des probabilités de changement d'état réel. Pour l'exemple, on va dire que c'est nous qui avons définie les probabilités du system donc nous y avons accès, mais il est important de distinguer les probabilités réelles des probabilités estimées. Les probabilités réelles sont fixes et ne changent pas. N'oublions pas que le system est immuable il a ses données et sa dynamique. Nous nous ne faisons que l'observer.\n",
    "\n",
    "Un élément important lié à cela est la **propriété de markov**. Pour qu'un system soit appelé un processus de markov, il faut que les états du system soient distinguables des uns des autres et uniques. Cela permet de ne dépendre que d'un seul et unique état pour prédire l'état suivant du system et donc de ne pas dépendre de tout l'historique quand nous voulons prédire l'état futur du system.\n",
    "\n",
    ">En probabilité, un processus stochastique vérifie la propriété de Markov si et seulement si la distribution conditionnelle de probabilité des états futurs, étant donné les états passés et l'état présent, ne dépend en fait que de l'état présent et non pas des états passés (absence de « mémoire »). Un processus qui possède cette propriété est appelé processus de Markov. Pour de tels processus, la meilleure prévision qu'on puisse faire du futur, connaissant le passé et le présent, est identique à la meilleure prévision qu'on puisse faire du futur, connaissant uniquement le présent : si on connait le présent, la connaissance du passé n'apporte pas d'information supplémentaire utile pour la prédiction du futur.   \n",
    "[*Propriété de Markov Wikipedia*](https://fr.wikipedia.org/wiki/Propri%C3%A9t%C3%A9_de_Markov)\n",
    "\n",
    "![proba](./img/proba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markov Reward Process\n",
    "\n",
    "> Une **transition** ici est le passage d'un état à un autre  \n",
    "> Un **épisode** est une suite de transition ex : (wake up => code => deploy => sleep)\n",
    "\n",
    "Maintenant que nous avons la base, introduisons le deuxieme élement j'ai nommé **les récompenses**. Nous allons ajouter une récompense (reward) a nos **transitions**. Les récompenses sont des nombres et peuvent être positif ou négatif, ce que nous tentons de faire, c'est de maximiser le nombre de récompenses acquis lors des **épisodes**.\n",
    "\n",
    "Pour chaque épisode, il y a une notion de rendement G, qui est le total des récompenses. L'objectif est de **maximiser** ce retour.\n",
    "![return](./img/return.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un élément important ici est ce qu'on appelle le **discount factor** généralement noté γ (gamma), c'est un nombre compris entre 0 et 1. C'est un peu compliqué, mais restez attentif et ouvert d'esprit : ) c'est important.\n",
    "\n",
    "Essayons, de comprendre le calcul ci-dessus. \n",
    "Pour chaque transition de notre **state space** nous avons une récompense que nous récoltons dans l'état t + 1 d'arrivée.\n",
    "Prenons un exemple avec un nouveau system plus proche de la vie réel.\n",
    "![reward](./img/reward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque élément de notre épisode, nous calculons le **rendement** comme étant une somme des récompenses futures.\n",
    "Prenons un exemple avec cette episode (wake up => code => deploy => sleep) : étant donné notre épisode où l'on commence dans l'état wake up, quelle est le rendement possible en étant dans cet état ? \n",
    "Autrement dit on va faire la somme des récompenses de tous les états de cet épisode soit  : -3 + 10 + 3 = 10.\n",
    "On vient donc de calculer le rendement de wake up pour cet épisode.\n",
    "\n",
    "Maintenant, il manque quelque chose. Les récompenses qui arrive dans le futur vont être multipliées par le **discount factor** (gamma) élevé à la puissance t comme ci-dessous. \n",
    "* Si gamma est égale a 1, le rendement sera égal à la somme de toutes les récompenses futures de l'épisode ce qui correspond à une visibilité parfaite de toutes les récompenses futurs de l'épisode.\n",
    "* Si gamma est égale a 0, alors le rendement sera égale à la récompense immédiate sans aucune récompenses futures et cela correspond donc à une visibilité très réduite sur le futur donc une sorte de myopie.\n",
    "\n",
    "Le gamma est un paramètre très important en RL. Pensez-y comme une mesure calculant jusqu'ou dans le futur nous regardons pour estimer le rendement. Plus gamma est proche de 1 plus notre vision est lointaine et plus gamma est proche de 0 plus vous êtes myope : ).\n",
    "\n",
    "![return](./img/return.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon, j'espère que je ne vous pas perdu, vous y êtes presque ! \n",
    "* Actuellement le calcule de ce rendement G n'est pas vraiment utile, car il est défini pour tous les épisodes spécifique que nous observons. Et donc ce rendement G calculé pour chaque épisode spécifique peut varier d'un épisode a l'autre même pour un même état.\n",
    "\n",
    "* Cependant si on calcule **l'espérance du rendement** pour chaque état du system (en faisant la moyenne d'énormément d'épisode) on peut avoir une valeur très utile appelée **value state** qui nous indique à quel point c'est bien d'être dans cet état-là.\n",
    "\n",
    "Pour chaque état du system la valeur v(s) est la moyenne des rendements G de cet état sur des épisodes différent.\n",
    "  \n",
    "![value](./img/valuefunction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Markov Decision Process\n",
    "> Ici une **transition** correspond à ceci : (etat => action => nouvelle etat => récompense) \n",
    "\n",
    "On arrive sur la dernière enveloppe, je pense que vous avez déjà une petite idée de comment étendre le processus de markov sous sa forme de processus de decision markovien. \n",
    "* Premièrement on va ajouter un set d'action qui doit être un nombre d'actions fini. C'est ce qu'on appelle **l'action space**.\n",
    "* Deuxièmement nous allons relier les actions avec le changement d'état et le gain de récompenses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![markov](./img/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le cas d'un processus de markov nous avions une matrice de transition. C'est un tableau à 2 dimensions qui contient les **probabilités réel** de passer d'un état 1 à un état 2 dans le system. Petit exemple avec le schéma si dessous. La probabilité d'être dans l'état s1 et de passé a l'état s2 est de p12.\n",
    "\n",
    "![process](./img/matrice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons rajouter les actions, il faut que l'on conditionne notre matrice de transition pour rajouter les actions. On va donc passer d'une matrice à 2 dimensions à une matrice à 3 dimensions. \n",
    "\n",
    "On n'observe plus passivement le system, on peut activement choisir une action à prendre à chaque changement d'état.\n",
    "Pour chaque état t on à une matrice ou la dimension représentant la profondeur représente les actions que l'on peut prendre et les deux autre dimensions représentent l'état source et l'état d'arrivé. Ci-dessous un schéma récapitulatif.\n",
    "\n",
    "![mdp_pro](./img/mdp_rpo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les actions que l'on prend peuvent maintenant influencer la probabilité de tomber dans l'état de destination.\n",
    "On pourrait se demander pourquoi les actions ne font pas tomber dans l'état de destination avec une probabilité de 100 %.\n",
    "\n",
    "Reprenons l'exemple avec le robot de tout à l'heure. Ce robot peut avoir des moteurs défectueux, et lorsque le robot prend une action il y a une probabilité que le moteur vrille et que le robot tombe dans la case qui est à coté. Si on demande au robot d'aller en haut il y a donc 90 % de chance que le robot tombe sur la case d'en haut et 10 % de chance qu'il reste où il est ou qu'il tombe sur une autre case. Dans la vie, les actions que l'on fait n'aboutissent pas forcément à ce qu'on espérait réellement. Il peut y avoir des imprévus.\n",
    "\n",
    "La récompense que l'on obtient dépend maintenant de l'action qui emmène vers l'état cible et plus seulement de l'état source.\n",
    "C'est similaire à la vie réelle. Lorsque l'on met des efforts dans quelque chose, on gagne généralement de l'expérience même si les résultats de nos efforts ne sont pas forcément formidables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bravo** d'avoir tenu jusqu'ici. On a couvert le plus important pour le RL et le MDP. La dernière chose dont nous devons parler rapidement est de la policy. Ensuite, vous aurez un vocabulaire complet et vous ne passerez plus pour des intrus lors de discussions sur le RL : )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Kaneky !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Après la théorie la pratique !** Ce premier Workshop a introduit une grande partie théorique, mais cela va permettre de ne pas le refaire sur les prochains Workshops. Dans ce premier pas vers la pratique, on va travailler sur un exemple très basique pour ne pas vous assommer d'un coup. Puis au fur et à mesure de la série des Workshops, on complexifiera. \n",
    "\n",
    "Ici, on va essayer d'utiliser la **value function** pour résoudre un labyrinthe. Il y aura un agent qui va se balader dans un labyrinthe où il y aura une target à aller chercher. La target rapporte une récompense de 10 et le feu un malus de 5. L'agent va se balader des milliers de fois dans le labyrinthe et apprendre petit à petit. Il y aura un dump terminal pour que vous puissiez voir le labyrinthe et le dump des valeurs de tous les états (V(s)) s'ajuster en temps réel. Vous comprendrez donc que la Value function représente donc le **cerveau** de l'agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer certains tools que nous aurons besoin ainsi que l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "from Envi import Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Première tache : définir les constantes ainsi qu'un namedtuple représentant les transitions (state, action, next_state, reward). Appelez ce namdetupe \"Transition\" et on l'utilisera comme un type nommée semblable à un vecteur de size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = # Define namedtuple\n",
    "BATCH_SIZE = # Define batch size\n",
    "GAMMA = # Define gamma\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelque chose de classique en RL est que l'agent doit posséder une **mémoire**. Cette mémoire va **stocker plusieurs transitions passées**. Comme cela, même si l'agent vient d'effectuer une transition, il l'aura en mémoire encore un bout de temps et continuera d'update la **value function** avec cette transition. Cela permet de s'entraîner plusieurs fois sur des transitions que l'on ne verrait que **très peu de fois**.\n",
    "\n",
    "On va donc crée un classe memory. \n",
    "Initialisez cette classe avec : \n",
    "* Une variable capacity représentant la capacité de stockage de transition de la mémoire (quand elle est pleine, la mémoire va oublier les transitions les plus anciennes.).\n",
    "* Une variable memory représentant un tableau de transition (la mémoire en elle-même)\n",
    "* Une variable position représentant l'offset de la mémoire autrement dit l'index dans le tableau de la dernière transition ajouté. Cela va nous permettre de rajouter des transitions au bon endroit et de retirer les plus anciennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        # Need some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On va créer une fonction push que l'on liera à la classe memory plus-tard. Cette fonction va prendre un *args: list en paramètre représentant les paramètres pour créer notre transition (param1 = stata, param 2 = next_state, etc ...). On commence par vérifier si la taille de la mémoire est inférieur à la capacité. Si c'est le cas, on ajoute un élément \"None\" à la mémoire ce qui représente une case vide.  \n",
    "\n",
    "* On se place dans la mémoire à l'index de la position grâce à notre variable interne et on ajoute à cet endroit une nouvelle transition qui prend en paramètre *args ce qui va décomposer notre liste d'arguments variadiques en liste et créer la transition puis la stocker dans la mémoire.  \n",
    "\n",
    "* On update la variable position avec la règle suivante :   \n",
    "On ajoute +1 à la position, mais si après l'ajout la position est supérieur à la capacité, la position revient a 0. Pour cela, vous pouvez utiliser un modulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(self, *args: list):\n",
    "    # Need some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On implémente deux autres fonctions qui nous serons utile. La fonction batch qui va nous renvoyer un sample de n transitions présent au hasard dans la mémoire de l'agent. Cela va nous permettre d'update la valeur des états avec ce qui est présent dans la mémoire de l'agent. Pour la fonction batch, utilisez random.sample() et pour la fonction len elle va nous renvoyez la taille actuel de la mémoire.\n",
    "\n",
    "Toutes ces fonctions seront liées à la classe memory plus-tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(self, batch_size):\n",
    "    return # Need some code\n",
    "\n",
    "def __len__(self):\n",
    "    return  # Need some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code permet de lier nos fonctions ci-dessus à la classe memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(Memory, 'push', push)\n",
    "setattr(Memory, 'batch', batch)\n",
    "setattr(Memory, '__len__', __len__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, on va s'attaquer a la classe principale. La classe Agent. \n",
    "* Cette classe prend en paramètre d'initialisation l'instance de l'environnement.\n",
    "* On crée une variable env pour stocker l'environnement reçu.\n",
    "* On crée une variable V et on l'initialise avec autant de 0 que l'environnement a de case (pour cela, on utilise la fonction get_map() de l'environnement)\n",
    "* Ensuite faite un appel à la fonction self.init_v() que l'on créera plus tard pour initialiser les valeurs des état ou se trouve les targets et les feux avec leur valeur par défaut. Ces valeurs seront égales à la récompense que donnent ces cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: Env):\n",
    "        # Need some code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici quelques fonctions utilitaires pour initialiser notre value table pour chaque case de l'environnement. Et deux fonctions pour save et load la valeur des états."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_v(self):\n",
    "    map = env.get_map()\n",
    "    for j in range(len(map)):\n",
    "        if map[j] == 'T':\n",
    "            self.V[j] = 10\n",
    "\n",
    "def save_v(self, path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        for v_f in self.V:\n",
    "            f.write(str(round(v_f, 4)) + \" \")\n",
    "\n",
    "def load_v(self, path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        self.V = f.read().split(\" \")\n",
    "        del self.V[len(self.V) - 1]\n",
    "        for k in range(len(self.V)):\n",
    "            self.V[k] = float(self.V[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la vie réelle, quand nous avons pris l'habitude de faire quelque chose, on le refait encore et encore parce qu'on a appris à faire cela. Mais quelques fois par hasard nous divergeons. Prenons un exemple : je suis un élève qui sort de l'école et pour rentrer chez moi, je prends toujours le même chemin, car c'est celui-ci que l'on m'a appris.\n",
    "Un beau jour, je me trompe par hasard de chemin et je découvre un chemin plus rapide pour rentrer chez moi et donc maintenant, j'utilise celui-ci.\n",
    "\n",
    "Et voilà ! Vous venez de voir le concept d'exploration et d'exploitation. Même si notre agent a appris quelque chose, il faut toujours qu'il y ait un pourcentage de chance qu'il prenne des actions aléatoires pour peu-être découvrir de nouvelles choses. Le pourcentage de prendre des actions aléatoires est ce qu'on appelle **l'epsilon**. Au début **epsilon** est très haut, car l'agent doit apprendre. Mais au fur et à mesure, l'epsilon diminue et du coup l'agent prend des actions en fonction de son cerveau et de ce qu'il a appris (soit la value fonction).\n",
    "\n",
    "Ici, on va implémenter la fonction **greedy_step**. C'est la fonction qui va prendre les décisions en fonction de ce que l'agent a appris.\n",
    "\n",
    "* On commence par créer un tableau d'actions. Il y a quatre actions possible : haut, bas, gauche, droite (1,2,3,4). On stock ce tableau dans une variable nommée actions.\n",
    "* Maintenant on va réfléchir. L'agent est à une position t. On veut savoir quelle action prendre en fonction de ce qu'on a appris. On sait que la value fonction nous permet de savoir à quel point, c'est bien d'être dans un état donné. Dans l'environnement que j'ai fait pour cet exercice, je vous ai créé une fonction predict(action) qui vous retourne un tableau à un seul élément qui représente la case sur laquelle l'agent se retrouverait s'il prenait cette action.\n",
    "* On sait également que chaque état possède sa **Value function**. On va donc itérer sur toutes les actions et regarder sur quel état nous tombons puis pour chaque état sur lequel nous pourrons tomber nous allons regarder la valeur de cet état grâce à self.V(S) puis nous allons prendre la valeur max que l'on a trouvé. On va choisir l'action qui nous amène a cet état et le retourné (cf la valeur de retour de la fonction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_step(self, state) -> int:\n",
    "    # Greedy step\n",
    "    actions = [1, 2, 3, 4]\n",
    "    v_max = None\n",
    "    vi = None\n",
    "    for i in range(0, 4):\n",
    "        a = actions[i]\n",
    "        predict_state = self.env.predict(a)[0]\n",
    "        if v_max is None or self.V[predict_state] > v_max:\n",
    "            v_max = self.V[predict_state]\n",
    "            vi = i\n",
    "    return actions[vi if vi is not None else 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on a la fonction générique qui va prendre **les actions**. En paramètre elle reçoit l'état actuel de l'agent et epsilon.\n",
    "\n",
    "* On va prendre un nombre entre 0 et 1 si ce nombre est inférieur à **epsilon** alors on renvoie un nombre entre 1 et 4 pour prendre une action aléatoire.\n",
    "* Si jamais le nombre est supérieur a epsilon on va appeler notre fonction **greedy_step** le wrap dans un tableau à un élément et retourner ce tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(self, a_state, eps_t: int) -> list[int] :\n",
    "    if random.uniform(0, 1) < eps_t:\n",
    "        return [random.randint(1, 4)]\n",
    "    else:\n",
    "        return [self.greedy_step(a_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rappel pour la suite :  \n",
    "Afin d'acquérir des récompenses, la fonction de valeur est un moyen efficace de déterminer la valeur d'être dans un état. On note cette fonction V(s). Elle mesure les futures récompenses potentielles que nous pourrions obtenir en étant dans cet état s . \n",
    "![rapp](./img/rapp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On se rappelle du calcul du rendement pour un état d'un épisode  \n",
    ">$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma ^ 2 R_{t+3} + ...$ \n",
    "\n",
    "> On se rappelle également que pour avoir la valeur de l'état on prend l'espérance de tous les rendements de cette etat sur énormément d'épisode différend.  \n",
    ">$V(s) = \\mathbb{E}[G_t | S_t = s]$ \n",
    "\n",
    "> On traduit l'expréssion. On update la valeur de chaque état grâce a cette formule qui reprend ce qu'on a dit juste au-dessus. On prend la valeur actuel et on lui rajoute la récompense à t + 1 + gamma * la différence de la valeur a l'état t + 2 et l'état actuel.  \n",
    "> On répète cette opération autant de fois qu'on a de transition ce qui va ajuster la valeur de l'état et va donc simuler l'Espérance du rendement total pour cette état comme montre le formule si dessus.  \n",
    ">$V(s) = V(s) + r + \\gamma * [V(s') - V(s)] $\n",
    "\n",
    "> Quelques choses basiques que l'on doit rajouter lorsque l'on update les paramètre d'apprentissage d'une IA est le learning rate (taux d'apprentissage) qui va permettre d'ajuster petit à petit la valeur de l'état plus-tot que de faire de gros ajustements qui seront instables.  \n",
    ">$V(s) = V(s) + \\alpha * [r + \\gamma * [V(s') - V(s)]] $\n",
    "\n",
    "> r = reward \n",
    "> s = state \n",
    "> s' = next_state \n",
    "> $\\gamma$ = gamma \n",
    "> $\\alpha$ = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tache : Implémentez la fonction learn. C'est la fonction principal responsable de l'update des values functions de chaque état.   \n",
    "* Récupérez des transitions de la mémoire de l'agent. (récupérez BATCH_SIZE transitions.). Grace à la fonction batch que vous avez implémentez tout à l'heure. Stoquez ce que vous avez récupéré dans une variable **\"transitions\".**\n",
    "* Il va falloir faire quelque prints et tester ce que vous avez récupéré de la fonction batch. Actuellement, vous avez récupéré un tableau de **Nametuple Transition**, mais nous voulons un seul **Nametuple Transition** formant un tableau. Pour cela, il va falloir l'opérateur * pour passer le tableau de **Nametuple Transition** que vous avez reçu de la fonction batch a la fonction zip pour justement unzip ce tableau de **Nametuple Transition** (zip est son propre inverse).\n",
    "* Ensuite il faut réutiliser * sur le retour du zip pour mettre ce retour sous forme de list. Et enfin, on créer un nouveau **Nametuple Transition** avec cette liste pour créer une seule transition formant un tuple de 4 éléments ou chaque élément est un tableau et on stoque ce **Nametuple Transition** dans une variable appelée **batch**.\n",
    "* **Faites des prints** pour voir ce que vous avez fait. Normalement, vous pouvez accéder à tous les états avec batch.state puis batch.reward pour les rewards etc... Qui doivent être 4 tableaux.\n",
    "* Il faut maintenant itérer sur batche.state, batch.next_state et batch.reward (vous pouvez faire un for avec un zip et récupéré un élément de chaque à chaque tour de boucle)\n",
    "* Ensuite Vous l'avez remarqué chaque unique states est un tableau à 1 élément donc dans la boucle for, initialisez trois variable **s, s_prime et reward** avec l'index 0 de chaque élément respectif (state, next_state et reward).\n",
    "* Une fois que vous avez cela implémenté l'update de self.V[s] avec la formule ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self):\n",
    "    # Need some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(Agent, 'init_v', init_v)\n",
    "setattr(Agent, 'save_v', save_v)\n",
    "setattr(Agent, 'load_v', load_v)\n",
    "setattr(Agent, 'greedy_step', greedy_step)\n",
    "setattr(Agent, 'take_action', take_action)\n",
    "setattr(Agent, 'learn', learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voici la dernière fonction : la fonction **main**. \n",
    "* On commence par créer une instance de l'environnement en lui passant le fichier txt représentant le labyrinthe.\n",
    "* On crée une instance de l'agent en lui donnant l'environnement.\n",
    "* On définie deux variables pour la boucle de jeux et le taux epsilon.\n",
    "* Ensuite on boucle le nombre de fois que l'on veut.\n",
    "* Toute les 10 itérations on diminue l'epsilon. \n",
    "* On récupère l'état actuel de l'agent.\n",
    "* On demande à l'agent quelle action il veut prendre. \n",
    "* On passe cette action dans l'environnement grâce à la fonction step qui va effectuer l'action\n",
    "* On push la transition que l'agent vient de faire dans sa mémoire\n",
    "* On render() pour afficher.\n",
    "* Tant que l'on n'a pas aux moins 128 transitions dans notre mémoire on stack les transitions.\n",
    "* Une fois 128 atteins l'agent va apprendre avec ce qu'il a dans sa mémoire et chaque fois que l'on rajoute une transition dans la mémoire l'agent va en supprimer une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env: Env = Env(\"./maze.txt\")\n",
    "    agent: Agent = Agent(env)\n",
    "    i = 0\n",
    "    eps = # Define epsilon\n",
    "    while i < # Define iteration:\n",
    "        if i % 10 == 0:\n",
    "            eps = max(eps * 0.9998, 0.05)\n",
    "        state: list = # Need state from env\n",
    "        action: list = # Need to take an action\n",
    "        next_state, reward = # Need to pass this action to step() function of env\n",
    "        # Need to push transition in memory's agent\n",
    "        env.render(agent.V, eps, 0.00000001)\n",
    "        i += 1\n",
    "        if i > 128:\n",
    "            agent.learn() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
